{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(self):\n",
    "    csv_file = pd.read_csv(\"datasets/image/img2smile.csv\")\n",
    "    smile_list = list(csv_file[\"smile\"])\n",
    "    ids = list(csv_file[\"imgid\"])\n",
    "    return smile_list,ids\n",
    "'''加载数据，csv文件是以imgid，smile存放的'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "\n",
    "def init_from_file():\n",
    "    csv_file = pd.read_csv(\"datasets/Chembridge-imageID-SMILE.csv\",names=[\"imgid\",\"smile\"])\n",
    "    smile_list = list(csv_file[\"smile\"])\n",
    "    ids = list(csv_file[\"imgid\"])\n",
    "    return smile_list,ids\n",
    "    \n",
    "    \n",
    "def tokenize(smiles):\n",
    "    regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "    smiles = replace_halogen(smiles)\n",
    "    char_list = re.split(regex, smiles)\n",
    "    tokenized = []\n",
    "    for char in char_list:\n",
    "        if char.startswith('['):\n",
    "            tokenized.append(char)\n",
    "        else:\n",
    "            chars = [unit for unit in char]\n",
    "            [tokenized.append(unit) for unit in chars]\n",
    "    tokenized.append('END')\n",
    "    return tokenized\n",
    "    \n",
    "def replace_halogen(string):\n",
    "    \"\"\"Regex to replace Br and Cl with single letters\"\"\"\n",
    "    br = re.compile('Br')\n",
    "    cl = re.compile('Cl')\n",
    "    string = br.sub('R', string)\n",
    "    string = cl.sub('L', string)\n",
    "    return string\n",
    "  \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "        \n",
    "def build_vocab():\n",
    "    words = []\n",
    "    smilelist,_ = init_from_file()\n",
    "    for smiles in smilelist:\n",
    "        token = tokenize(smiles.lower())\n",
    "        for tk in token:\n",
    "            words.append(tk)\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab = build_vocab()\n",
    "    with open(\"datasets/image/total_vocab.pkl\",'wb') as f:\n",
    "        pickle.dump(vocab,f)\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/image/total_vocab.pkl\", 'rb') as f:\n",
    "    vocab_total = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<start>': 1,\n",
       " '<end>': 2,\n",
       " '<unk>': 3,\n",
       " 'o': 4,\n",
       " '=': 5,\n",
       " 'c': 6,\n",
       " '(': 7,\n",
       " '1': 8,\n",
       " 'n': 9,\n",
       " ')': 10,\n",
       " '2': 11,\n",
       " 'f': 12,\n",
       " 'END': 13,\n",
       " '3': 14,\n",
       " '[h]': 15,\n",
       " '[c@]': 16,\n",
       " '[c@h]': 17,\n",
       " '[c@@h]': 18,\n",
       " '[c@@]': 19,\n",
       " 'l': 20,\n",
       " 's': 21,\n",
       " '#': 22,\n",
       " '/': 23,\n",
       " '\\\\': 24,\n",
       " '4': 25,\n",
       " '5': 26,\n",
       " '[n+]': 27,\n",
       " '[o-]': 28,\n",
       " '6': 29,\n",
       " 'b': 30,\n",
       " 'r': 31,\n",
       " '[n-]': 32}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_total.word2idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "词典构造\n",
    "在这一部分我们采用了 自己构造的token函数，并将卤素原子替换为单字符，以免识别错误，\n",
    " 将smile序列分词之后。构造词典，这里额外增加了pad，unk，star end 符号，在seq2seq 中 输入的是star-seq 输出的是seq-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class smile_dataset_loader(Dataset):\n",
    "    def __init__(self,vocab,transform=None):\n",
    "        csv_file = pd.read_csv(\"datasets/image/img2smile.csv\")\n",
    "        smile_list = list(csv_file[\"smile\"])\n",
    "        ids = list(csv_file[\"imgid\"])\n",
    "        self.data=smile_list\n",
    "        self.ids = ids\n",
    "        self.vocab =vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        data_id = self.ids[index]\n",
    "        caption = self.data[index]\n",
    "        vocab = self.vocab\n",
    "        image = Image.open(os.path.join(\"datasets/image/train/\",str(index+1)+\".png\"))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        tokens = tokenize(caption)\n",
    "        captions = []\n",
    "        captions.append(vocab('<start>'))\n",
    "        captions.extend([vocab(token) for token in tokens])\n",
    "        captions.append(vocab('<end>'))\n",
    "        target = torch.Tensor(captions)\n",
    "        return image, target\n",
    "            \n",
    "\n",
    "    \n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(smiles):\n",
    "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "        smiles = replace_halogen(smiles)\n",
    "        char_list = re.split(regex, smiles)\n",
    "        tokenized = []\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                tokenized.append(char)\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokenized.append(unit) for unit in chars]\n",
    "        tokenized.append('END')\n",
    "        return tokenized\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "    images = torch.stack(images, 0)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "    \n",
    "def get_loader(vocab, transform):\n",
    "    data = data_Loader(vocab=vocab,transform=transform)\n",
    "    data_lodaer = torch.utils.data.DataLoader(dataset=data,batch_size=40,\n",
    "                                                 shuffle=True,\n",
    "                                                 collate_fn=collate_fn)\n",
    "    return data_lodaer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据加载 继承了dataset构造数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=False)\n",
    "        resnet.load_state_dict(torch.load(\".torch/models/resnet152-b121ed2d.pth\"))\n",
    "        modules = list(resnet.children())[:-2]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, 100)\n",
    "        self.bn = nn.BatchNorm1d(100, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, max_seq_length=50):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(100, 128, 2, batch_first=True)\n",
    "        self.linear = nn.Linear(128, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        self.attention = Attention()\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def init_hidden_state(self):\n",
    "        init_c = \n",
    "        init_h = \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/45], Loss: 3.4441, Perplexity: 31.3149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-0c03cd5d3971>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \"datasets/images\", 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-119-0c03cd5d3971>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Forward, backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-55fb4b735472>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;34m\"\"\"Extract feature vectors from input images.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[1;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[0;32m    836\u001b[0m     \"\"\"\n\u001b[0;32m    837\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def main():\n",
    "    transform = transforms.Compose([ \n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "    with open(\"datasets/image/vocab.pkl\", 'rb') as f:\n",
    "        vocab_train = pickle.load(f)\n",
    "\n",
    "    data_loader = get_loader(vocab_train,transform) \n",
    "    encoder = EncoderCNN().to(device)\n",
    "    decoder = DecoderRNN(len(vocab_train)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "    \n",
    "    total_step = len(data_loader)\n",
    "    for epoch in range(6000):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "            \n",
    "            # Set mini-batch dataset\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            \n",
    "            # Forward, backward and optimize\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch+1, 10, i+1, total_step, loss.item(), np.exp(loss.item()))) \n",
    "                \n",
    "            # Save the model checkpoints\n",
    "            if (i+1) % 50000 == 0:\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pickle \n",
    "import os\n",
    "from torchvision import transforms \n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image\n",
    "def sample_smlie(sample_image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(\".torch/models/total_vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    encoder = EncoderCNN()\n",
    "    decoder = DecoderRNN(len(vocab))\n",
    "    \n",
    "    encoder.load_state_dict(torch.load(\".torch/models/encoder-12-5000.ckpt\",map_location='cpu'))\n",
    "    decoder.load_state_dict(torch.load(\".torch/models/decoder-12-5000.ckpt\",map_location='cpu'))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image = load_image(sample_image, transform)\n",
    "    image_tensor = image\n",
    "    feature = encoder(image_tensor)\n",
    "    sampled_ids = decoder.sample(feature)\n",
    "    sampled_ids = sampled_ids[0].numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "    \n",
    "    # Convert word_ids to words\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "    sentence =  re.sub('\\s+', '', sentence).strip()\n",
    "    # Print out the image and the generated caption\n",
    "    print (sentence)\n",
    "    image = Image.open(sample_image)\n",
    "    plt.imshow(np.asarray(image))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start>CC1=CC=C(C2=CC=C(C3=NC=CN3CCCO)C=C2)O1<end>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt0VPXd7/H3NxcCIhKRgOEmCCkYPQhJRAtqmwIqiBfqpWAL9Fm6kvapJVjbSp9SCU9v9Gaj7WklLa4zXo6AR2mJC+wTwSpqAcP9ZgAFAQkBuYMVSfI9f8yeMCG3nbntSfJ9rTUrM7/5zd7fGcgnv733b/YWVcUYY5qT4HUBxpjWwcLCGOOKhYUxxhULC2OMKxYWxhhXLCyMMa5ELSxE5DYRKReRXSIyM1rrMcbEhkRjnoWIJAI7gLHAfuA9YLKqbov4yowxMRGtkcUIYJeqfqiqnwMLgLuitC5jTAwkRWm5vYF9QY/3A9c31rl79+7av3//KJVijAFYu3btJ6qaFurroxUW0kBbne0dEckD8gD69etHWVlZlEoxxgCIyEfhvD5amyH7gb5Bj/sAB4I7qGqxquaoak5aWshhZ4yJkWiFxXtAhogMEJEOwCRgSZTWZYyJgahshqhqlYg8DPwDSASeUdWt0ViXMSY2orXPAlVdCiyN1vKNMbFlMziNMa5YWBhjXLGwMMa4YmFhjHHFwsIY44qFhTHGFQsLY4wrFhbGGFcsLIwxrlhYGGNcsbAwxrhiYWGMccXCwhjjioWFMcYVCwtjjCsWFsYYVywsjDGuWFgYY1yxsDDGuGJhYYxxxcLCGOOKhYUxxhULC2OMKxYWxhhXLCyMMa5YWBhjXLGwMMa4Eta1TkVkD3AKqAaqVDVHRLoBC4H+wB7gflU9Fl6ZxhivRWJkkauqw1Q1x3k8E1iuqhnAcuexMaaVi8ZmyF2Az7nvA+6OwjqMMTEWblgo8D8islZE8py2nqpaAeD87BHmOowxcSCsfRbAKFU9ICI9gFIRed/tC51wyQPo169fmGUYY6ItrJGFqh5wfh4CFgMjgEoRSQdwfh5q5LXFqpqjqjlpaWnhlGGMiYGQw0JEOotIl8B94BZgC7AEmOZ0mwb8PdwijTHeC2czpCewWEQCy/m/qvqaiLwHLBKRB4G9wH3hl2mM8VrIYaGqHwLXNtB+BBgdTlHGmPhjMziNMa5YWBhjXLGwMMa4YmFhjHHFwsIY44qFhTHGFQsLY4wrFhbGGFcsLIwxrlhYGGNcsbAwxrhiYWGMccXCwhjjioWFMcYVCwtjjCsWFsZ4pKqqigceeIC3337b61JcsbAwxiNz5szhxRdfJDc31+tSXLGwMMYjv/zlLwH/CKM1sLAwJsZqamq45ZZbqK6upmfPnhw4cMDrklyxsDAmxn7xi19QWlpKQkICzz33HOnp6V6X5Eq4FxkyxrRA//79+eijjwD4/PPPSUxM9Lgi92xkYUyMqGptUKSlpbWqoAALC2Ni5re//S0AIsL8+fM9rqblbDPEmBi48sor2b17NyNGjGDlypV06NDB65JazEYWxsTA7t27AVi4cGGrDAqwsDAm6v7whz8A/s2P/v37e1tMGGwzxJgoOX36NDk5OZSXlzN06FBWr17tdUlhsZGFMVHy7W9/m/LycgAWLVpEx44dPa4oPM2GhYg8IyKHRGRLUFs3ESkVkZ3Oz0uddhGRp0Rkl4hsEpGsaBZvTLzaunUrzz//PAD5+fkMHjzY44rC52Zk8X+A2y5omwksV9UMYLnzGGAckOHc8oA/R6ZMY1qPM2fOcP/99wNwzTXX8MQTT3hcUWQ0u89CVd8Skf4XNN8FfNm57wP+CTzmtD+rqgqsEpFUEUlX1YpIFWxMNOzZs4cf/vCHzfa79NJLm+3z5ptvUl5eTufOnVm0aBEXXXRRJEr0XKg7OHsGAkBVK0Skh9PeG9gX1G+/01YvLEQkD//og379+oVYhjHhO3PmDAMGDIj4ck+ePElCQtvZLRjpoyHSQJs21FFVi4FigJycnAb7GBMLr776KgBXXHEFv/nNb5rse+zYsSafV1VmzpzJ8ePHWbVqFSNHjoxYnV4LNSwqA5sXIpIOHHLa9wN9g/r1AVrH929Nu/XSSy8B8NBDD3HfffeFvbx169ZRXFxMSUlJmwqLUMdIS4Bpzv1pwN+D2qc6R0VuAE7Y/goTzxYuXMjLL7/M0KFDmTVrVkSWWVRUROfOnZk7dy47d+6MyDLjgZtDpy8C/wIGi8h+EXkQmAuMFZGdwFjnMcBS4ENgF/AX4D+jUrUxERIYVURiRBHQqVMnxowZA5zfxGkLxH/gwls5OTlaVlbmdRmmnVm4cCGTJk0C/PsaAFauXMn27dtJTU1FpKFdcOdddNFFpKSk1GkLhERlZSUZGVfw0EPD+PWvl5GU1PxRlGgTkbWqmhPq6226t2m3AqOKYM8//zzFxcUhLzMQOj179uSf/0xCdTUnTy6jW7cHQl5mvGg7x3WMaaHXXnutXlskR9qqZwA4caIkYsv0koWFabfOnPH/Mv/sZz+rbSsuLkZVQ74FS0kZBMDRowti96aiyMLCtEuffvpp7f177rknKuvo2vX2qCzXKxYWpl0qKfFvGlx77bUMGTIkKutITb0jKsv1ioWFaXc+/fRTHnzwQQAWLIjeJkKXLqPp1m0yACdOLIvaemLFjoaYdqekpIQzZ85EdVQRcPnlM+nZ83tcdFHIRyzjho0sTLvT0Khi8uTJdO3aNeLr6tRpaJsICrCwMO1Q4ChI8KhiwYIFnDx5kqlTp9Y+H0lr1wpr1woffZTPoUNPRnz5sWBhYQyQkZEBwHPPPUdOTg4bN26M2LJPnFhGdraSne0/tLpv3wzOnt0VseXHioWFaVcCmx7XXnttnfYdO3agqlx88cW8//77DBs2jIkTJ3L06NGw17lr1/ja+1dcMY9rrtnJZ5+1vi+YWViYdqW5L46VlZUxbNgwAP72t78xbNgw3n777bDW2b17Xp2RRErKILp2HRfWMr1gYWHalbvuugvwn/quIYMHD2bVqlVMnz4dgH379pGbm0t1dXXI67ziinls2ZJBefmokJcRDywsTLsSOJFuaWkpycnJFBYWUlNTU6dPSkoKTz75JHv37uXmm2+mqqqKpKQkevTo0dAiXcnOVgYPfofy8lGsXSu2z8KYeNexY0cWL15Mt27dqKqqYs6cOYwdO7bBvn379mXFihXMnj0bgMOHD1NQUMDnn3/eonUGB8Pgwe/QvXseBw82ffq+eGRhYdqdu+++myNHjjB79mwSExNZsWIFCQkJFBQU1OubmJhIYWEhhw8fZsKECTz11FOkpKRwySWXNLjsc+cOcvz4K3Xa9uyZVudxp06ZkXszMWRhYdqtwsJCSktL6dWrF6rKU089xQcffNBg3+7du7NkyZLak92cOnWqwX7bt2fx4YeTOHNmTW1bx47X1E73PnFiGceOLeLyy38Q4XcTfRYWpl3Lzc1lw4YNtY+zsrJ48cUXG+wrIrz77rtkZGQwcODAOs/V1HzGvn3TOXeuAtVz7NnzH7XPdeqUyfHjf2PtWuHgwZ/Rv7+v9uvrrYmdVq8NKi0tZfny5UyfPp1evXp5XU6rUVZWxnXXXVf7eOPGjQwdOrTZ1x058mztpkZq6t1cccVfSUq6LGp1hirc0+rZyKKNWbVqFfPnz+dXv/oVvXv3plevXtxxxx28/vrrdc7hYOrLycnh61//eu3j66+/niefbHxq9ieffMKjjz7EBx88BEBCwkUMHLg4LoMiIsI5K1CkbtnZ2RqSPFRBdWSRqu5UHek8LtoZ1Gfk+T47G11Sm1BTU6PXX3+9iojiv7hTnVtSUpJmZmbqvHnzdMuWLV6XG7d8Pp927ty59nObOHFig/169+6tgBYUfFm3br1aP/10U4wrbRmgTMM5C1g4L47ULeSwUFXNWxr0wAmMgKKRF/Tlgv4t88knn+jmzZt106b4+09x6tQp7dWrlwI6a9YsVVWtqqrSLVu2qM/n0+zsbE1ISGgwRABNT0/X++67T4uKivTf//63x+8mPkyfPr02eJOSknT27Nn6/vvva1ZWVu3n1qNHD3311Ve9LtUVC4tGw+KC4FBVXZqnSl7Ty/vsM9W331ZdsEBLiot18uTJetNNN2nHjh1r/4N897vf1Tlz5ui+fftCrzvCfvzjH9fWd+rUqQb7nDhxQl9//XW94447tEePHo0GR8eOHXXkyJG6YMECPXnyZIzfSXxZvHixduvWrfazCR5xjB07Vg8cOOB1ia5ZWDQWFkvzVLlgZLGzyHnLQW6/3d/WwG3ul75U55eoS5cuetVVV2liYmJtW7wI1DNixIiQXr9lyxadN2+eTpkypfavaUpKit5000369NNPR7ja1mf27NkqIpqYmKgDBw7U9evXe11Si1lYNPSLrtpwWOjS+mGRl1f3tUlJqjfcoDpxoq4pLNRnn31Wly9fXuevdWFhYe0v57vvvht67RHy9tuqOTmzVER05cqVYS/v0KFD+uqrr9a+x4cffjgCVbZ+mZmZCuibb77pdSkhsbAId2SxYoXqH/+oumaN6oEDqtXVza6yurpac3NzFdABAwbo8ePHQ68/TNXVqllZ/rf1ve99VKfGcAXC4t577w17WW3Bddddp4CuWbPG61JCEm5YtN1zcI77AYzMqNu2cxuQV7ctN9d/a4GEhARWrFjBK6+8wj333ENqaqo/eWPs1CkYPBgqKuC//xt+8pN+ABw9epTu3bvXqWnMmDGMGjWK7Oxs7rijZWedXr9+fUTrbq3OnTsHQHJysseVeMPNhZGfEZFDIrIlqK1QRD4WkQ3ObXzQcz8SkV0iUi4it0ar8OYNgvsvuNz934oh7+6IreGrX/1q7f3y8vKILdetn/7UHxQAjz56vn3z5s31+r7++uvMmTOHO++8k8zMTL75zW+yYcMGqqqqml1PRWAl7Vzga+qJiYkeV+KR5oYewM1AFrAlqK0Q+H4DfTOBjUAKMAD4AEhsbh1RO3SqO88/XzRSmz0SEoKrrrpKAc3OztazZ89GfPlNSUmpu5sm2MmTJ3Xu3Lk6YcIETUtLa/TIR3JysmZnZ6vP56s39yJ4z785v8+itc5RIRb7LID+LsPiR8CPgh7/A/hic8sPKyziwBNPPKGAJiQkaGlpaczWO2WKakKCf3dLS5SVlWlRUZFmZmY2OIEr4MYbb7SwCJKRkaGA7tixw+tSQhJuWIQz3fthEdnkbKYEriffG9gX1Ge/01aPiOSJSJmIlB0+fDiMMrw3Y8YMAGpqavjGN74Rs/U++yyUl0PQ1xlcyc7OpqCggK1bt3L06FFKS0uZMGEC3bp1q9Pv8ssvj2C1rV/gJDkJCe3zWxKhvus/AwOBYUAF8DunXRro2+CeP1UtVtUcVc1JS0sLsYz4ICL84x//QESorKzkj3/8Y0SWm58PIjAq6Gxsy5bVbRsU5pcXU1NTGTNmDCUlJRw5cqTOTtHhw4eHt/A2xnZwhkBVK1W1WlVrgL8AI5yn9gN9g7r2AQ6EV2LrcMstt/DII48A8P3vfz8ip5KfNw+KiuDdd8+3jRvnb3vnnbAX36z09PTor6QVae87OEMKCxEJ/l80EQgcKVkCTBKRFBEZAGQAay58fVs1d+5cRowYwdmzZ3nggQcittyioogtqkVsM6Qu2wxphoi8CPwLGCwi+0XkQeDXIrJZRDYBucAjAKq6FVgEbANeA76jqqGfFrmVSU5OZvXq1QBs27aNRYsWRWS5BQX+TY9Yy8rKiv1K45hthjRDVSerarqqJqtqH1Wdr6pTVPV/qepQVb1TVSuC+v9cVQeq6mBVbf2Xjg7B5Mn+K2d/61vfYu/evRFbbhOnVoiKtLQ0kpL88/ZaepLatsg2Q0zE/elPf6J///4cO3aMKVOmNHrNiX//G3btgrfegt/8BmbMgHvvheLi+n2LivzPR5uq4vP5AP9wO7Dz+dChQ9FfeZwL/Du2182Qtjvd20Opqans3r0bEeGtt96q/etcX8NTxEUg74JZ6QUF8IUv+J+L9D6M06dPc/XVV9cZBR07dowZM2YwfPhwKioqWL9+PX369InsiluZwGZIhw4dPK7EGxYWUZSbm0tFRQX79+/n9OnT9Z7v0AHS06FPH/9ciT59oFcvuOmmhpc3bhyMHNnwc6GorISnnoJ167bUCYpHHnmkdip74IiITfm2HZwWFlG0YsWKiC/znXfC33dx772weDGcvxDXDTz99NNMmzaNjh071ukbmGthXyazHZztMyJbiVGj/PspIn0k5OWXzwfF8OHg80F+fn69oIDzh08PHjwY2SJaIRtZmLjV2MSrBi6c1SKXXw4HD8KSJdDct9VtM8SvvR8JARtZtBm9e8POne76VlT4v6vaVFCcPXuWIUOGMMqZV7569WqSk5PJz8+PQLXuVVRUkJ+fz6BBg+jbty+JiYmejHICFxX62te+FvN1x41wvoUWqVtr/9ap1554QlVEdf78yCxvzpw5TX6tfd68eVpVVRWZlTWhqKhIL7nkknrr79y5s86ePTvq61dVXbdund5888216z5x4kRM1hsNtPvT6hldtUr1hRfCX87u3aqPP/5CnV/Myy67TB9//HG99dZb67QPGzYs/BU2YcmSJXXWl5ubq+PHj6/T5vP5tKamJmo1TJ8+vc7JmS+77LKYhGS0WFiYsKxcqTphgtaeREdEdejQoerz+Rrsf/bsWe3SpUvtL1CnTp30sccei0gtR48e1enTp9cuu1u3blpUVFTvF9Tn82nPnj0V0JycZSqiumdPRErQ0lLVq68OfB7+OsaMGROZhXvMwsLUA6ojg85VvHRp/bZz586f6Df4Nnp088v/+OOPdcqUKRH9K+/z+eps+iQlJenhw4cb7X/q1CkdOXJibd0XXaT62GOqjVwyxZUdO+p+FllZX9bXXnst9AXGGQsLU09Rkf92YduFwH9qvilTQltPTU1Nnb/yOGcLq6ysdPX6vXv31gudQYMGtbiOFSvq/pLn5qrW1Jy/ykNzwXns2PnXJiaq5uerHjrU4jLinoWFqScQDMEB0VBYzJ6t2sQfb9dOnz6tKSkptb/wqampOnfu3GZfE3yVt8BOy88++yykGnw+1csv9/+PDt73WdTA1R8aC05Q3bAhpNW3CuGGhR06bcOa++JZYSF07x7+ejp37szmzZuZMGECAMePH2fmzJksXbq0wf4vvfQSmZmZfPbZZ4D/TGO7du2isLCQlJSUkGqYOtX/pbzZs+EHP6j7nJvv0mRm+ienXXttSKtvFyws2rCiotidByMjI4OSkhJUlaKiIrp27crtt9+OiNChQwd27NhBfn4+iYmJ3H///ezduxefz0dNTQ01NTUROdFO587+AOzcuW67m/OBbN3qDxzTOAuLNqygwP/Fs1ifB6OgoIDy8vLaadHnzp1jyJAhFBcXU1NTQ8eOHZk1axZTp05FYnhWn1h/Dm2NhUUbN2tWbM6DcaGePXvy3nvvceONN9Y+Dti2bRs//elPY1pPrM4H0pZZWLRx48ZBI7sOoi4rK4uVK1dy4MABDh48SHp6OqrKgAEDYl5LQYH/c/Di9IRthYVFOzBunLfrj5dva0b6fCDtjYVFG9TQt1LD/aZqOOIlLCA2l1Boq7z/1zNtnldhEa3zgbRXdj4LE3X++UDE9MgHRO98IO2VhYWJung4K/aePXDkCHTtGv4lH9sr2wwxURcYWXgZFj/5CeTkQEaGZyW0ehYWJuq82gypW4Nnq24zLCxM1MXD+Ssbuc6TaQELCxN1NrJoG9xcGLmviLwhIttFZKuIFDjt3USkVER2Oj8vddpFRJ4SkV0isklE7Oq67Vw8jCzOXyPFhMrNyKIKeFRVrwJuAL4jIpnATGC5qmYAy53HAOOADOeWB/w54lWbViUeRhYWFuFr9tCp+q+QXuHcPyUi24HewF3Al51uPuCfwGNO+7POyTZWiUiqiKRr0JXWTfsSD2HRq9eLfPGLgUsIPOJZHa1Zi+ZZiEh/YDiwGugZCABVrRCRHk633sC+oJftd9osLNqpeNgM2bt3If/619+dRxYWoXC9g1NELgZeBmao6smmujbQVm/3kojkiUiZiJQdPnzYbRmmFYqHkYXaHs6wuQoLEUnGHxQvqOorTnOliKQ7z6cDh5z2/UDfoJf3AQ5cuExVLVbVHFXNSUtLC7V+0wpYWLQNbo6GCDAf2K6qTwQ9tQSY5tyfBvw9qH2qc1TkBuCE7a9o3+JhM6TaJlqEzc0+i1HAFGCziGxw2v4LmAssEpEHgb3Afc5zS4HxwC7gU+A/IlqxaXUCf9Wrqqo8q+HUqVOerbvNCOfU4JG62aUA2r7U1NQ6FySKherqavX5fNqjRw8FNDExUTdu3BiTdccj7FIApjXo06dP7f1p06bxla98JerrHD58ONOmTePQIf/utC996UshX2rA2HRvEyMbN27E5/PRo4f/CPsbb7zB1KlTqaysjPi69u3bx9SpU9m0aVNtm8/nY/ny5QwePDji62s3whmWROpmmyHtS3l5uU6YMKHOZQsXLVoU1jJramp00aJF2q9fv9pl3nffffrRRx9FqOrWD7t8oWmtrr766jqBMXr0aN20aVNIyxo1alSdZeXk5ES42tYv3LCwzRDjmfXr1zNv3jy6O9dQXL58OVlZWbRkkl5FRQX5+fm8E3QOvXnz5rF69eqI19veWVgYzyQnJ5OXl8fhw4fZtm0bt912G1VVVfTo0QMRafRaqQAlJSWICL169aK4uJgJEybwwQcfoKrk5eXFxZnE2xr7RE1cuOqqq1i2bBlLliypbbv99tsZO3Ys27Ztq9N3/Pjx3HnnnbWPhwwZQklJCVdeeWXM6m2PxL8p462cnBwtKyvzugwTRyoqKujTp0/tZQS6du1KdXU1p0+fBvyjksrKSi699FIvy2xVRGStquaE+nobWZi4lJ6eznvvvcdNN90EwIkTJ2qDYvTo0axbt86CIsZsZGHiXklJCX/96185fvw4Bw4cYOfOnV6X1CqFO7KwsDCmnbDNEGNMTFhYGGNcsbAwxrhiYWGMccXCwhjjioWFMcYVCwtjjCsWFsYYVywsjDGuWFgYY1yxsDDGuGJhYYxxxcLCGOOKhYUxxhULC2OMKxYWxhhX3FxFva+IvCEi20Vkq4gUOO2FIvKxiGxwbuODXvMjEdklIuUicms034AxJjbcXEW9CnhUVdeJSBdgrYiUOs/9XlV/G9xZRDKBScDVQC/gdRH5gqraNe+NacWaHVmoaoWqrnPunwK2A72beMldwAJVPauqu4FdwIhIFGuM8U6L9lmISH9gOBC43NPDIrJJRJ4RkcCplnsD+4Jetp+mw8UY0wq4DgsRuRh4GZihqieBPwMDgWFABfC7QNcGXl7vrMAikiciZSJS1pLL1RljvOEqLEQkGX9QvKCqrwCoaqWqVqtqDfAXzm9q7Af6Br28D3DgwmWqarGq5qhqTlpaWjjvwRgTA26OhggwH9iuqk8EtacHdZsIbHHuLwEmiUiKiAwAMoA1kSvZGOMFN0dDRgFTgM0issFp+y9gsogMw7+JsQfIB1DVrSKyCNiG/0jKd+xIiDGtX7Nhoapv0/B+iEYvca2qPwd+HkZdxpg4YzM4jTGuWFgYY1yxsDDGuGJhYYxxxcLCGOOKhYUxxhULC2OMKxYWxhhXLCyMMa5YWBhjXLGwMMa4YmFhjHHFwsIY44qFhTHGFQsLY4wrFhbGGFcsLIwxrlhYGGNcsbAwxrhiYWGMccXCwhjjioWFMcYVCwtjjCsWFsYYVywsjDGuWFgYY1yxsDDGuGJhYYxxpdmwEJGOIrJGRDaKyFYRmeO0DxCR1SKyU0QWikgHpz3FebzLeb5/dN+CMSYW3IwszgJfUdVrgWHAbSJyA/Ar4PeqmgEcAx50+j8IHFPVQcDvnX7GmFau2bBQv9POw2TnpsBXgP/ntPuAu537dzmPcZ4fLSISsYqNMZ5IctNJRBKBtcAg4H8DHwDHVbXK6bIf6O3c7w3sA1DVKhE5AVwGfHLBMvOAPOfhaRE5cmEfj3XH6mlKvNUD8VdTvNUzOJwXuwoLVa0GholIKrAYuKqhbs7PhkYRWq9BtRgoDjwWkTJVzXFTTyxYPU2Lt3og/mqKx3rCeX2Ljoao6nHgn8ANQKqIBMKmD3DAub8f6OsUlwR0BY6GU6QxxntujoakOSMKRKQTMAbYDrwB3Ot0mwb83bm/xHmM8/wKVa03sjDGtC5uNkPSAZ+z3yIBWKSqr4rINmCBiPwMWA/Md/rPB54TkV34RxSTXNZS3HyXmLJ6mhZv9UD81dSm6hH7o2+MccNmcBpjXPE8LETkNhEpd2Z8zvSohj0isllENgT2GItINxEpdWaolorIpVGu4RkROSQiW4LaGqxB/J5yPrNNIpIVo3oKReRj53PaICLjg577kVNPuYjcGoV6+orIGyKy3ZlJXOC0e/IZNVGPJ59RTGZaq6pnNyAR/5yNK4EOwEYg04M69gDdL2j7NTDTuT8T+FWUa7gZyAK2NFcDMB5Yhv8w9Q3A6hjVUwh8v4G+mc6/XQowwPk3TYxwPelAlnO/C7DDWa8nn1ET9XjyGTnv82LnfjKw2nnfi4BJTvvTwLed+/8JPO3cnwQsbG4dXo8sRgC7VPVDVf0cWIB/Bmg8CJ6JGjxDNSpU9S3qH2JurIa7gGfVbxX+w9jpMainMXcBC1T1rKruBnbh/7eNZD0VqrrOuX8K/xG53nj0GTVRT2Oi+hk57zOqM629Dova2Z6O4JmgsaTA/4jIWmdmKUBPVa0A/38MoIcHdTVWg5ef28POsP6ZoE2zmNbjDJmH4//r6flndEE94NFnJCKJIrIBOASU0oKZ1kBgpnWjvA4LV7M9Y2CUqmYB44DviMjNHtTQEl59bn8GBuL/QmEF8LtY1yMiFwMvAzNU9WRTXWNRUwP1ePYZqWq1qg7DP0lyBBGYaR3M67Cone3pCJ4JGjOqesD5eQj/dPYRQGVg2Or8PBTrupqowZPPTVUrnf+QNcBfOD+Mjkk9IpKM/xfzBVV9xWn27DNqqB6vPyOnhqjMtPY6LN4DMpwZmWsDAAABE0lEQVQ9th3w72hZEssCRKSziHQJ3AduAbZQdyZq8AzVWGqshiXAVGeP/w3AicBQPJou2OafiP9zCtQzydnDPgDIANZEeN2Cf8LfdlV9IugpTz6jxurx6jOSWMy0juQe4hD34o7Hvyf5A+DHHqz/Svx7qTcCWwM14N9+Ww7sdH52i3IdL+Iftp7Dn/oPNlYD/iFk4Nu/m4GcGNXznLO+Tc5/tvSg/j926ikHxkWhnhvxD5M3ARuc23ivPqMm6vHkMwKG4p9JvQl/QD0e9P97Df4dqi8BKU57R+fxLuf5K5tbh83gNMa44vVmiDGmlbCwMMa4YmFhjHHFwsIY44qFhTHGFQsLY4wrFhbGGFcsLIwxrvx/3Kf9P+ZxkywAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = sample_smlie(\"datasets/image/test/1603.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300 at 0x26145C1EB00>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('1e.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    " \n",
    "template = rdkit.Chem.MolFromSmiles(x)\n",
    "tmp=AllChem.Compute2DCoords(template)\n",
    "Draw.MolsToImage(template)\n",
    "#Draw.MolToFile(template,'1e.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Mol' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-5e7a6e865792>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMolsToImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rdkit\\Chem\\Draw\\__init__.py\u001b[0m in \u001b[0;36mMolsToImage\u001b[1;34m(mols, subImgSize, legends, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlegends\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[0mlegends\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m   \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGBA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubImgSize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubImgSize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Mol' has no len()"
     ]
    }
   ],
   "source": [
    "Draw.MolsToImage(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def get_equal_rate_1(str1, str2):\n",
    "     return difflib.SequenceMatcher(None, str1, str2).quick_ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<pad>',\n",
       " 1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '<unk>',\n",
       " 4: 'o',\n",
       " 5: '=',\n",
       " 6: 'c',\n",
       " 7: '(',\n",
       " 8: '1',\n",
       " 9: 'n',\n",
       " 10: ')',\n",
       " 11: '2',\n",
       " 12: 'f',\n",
       " 13: 'END',\n",
       " 14: '3',\n",
       " 15: '[h]',\n",
       " 16: '[c@]',\n",
       " 17: '[c@h]',\n",
       " 18: '[c@@h]',\n",
       " 19: '[c@@]',\n",
       " 20: 'l',\n",
       " 21: 's',\n",
       " 22: '#',\n",
       " 23: '/',\n",
       " 24: '\\\\',\n",
       " 25: '4',\n",
       " 26: '5',\n",
       " 27: '[n+]',\n",
       " 28: '[o-]',\n",
       " 29: '6',\n",
       " 30: 'b',\n",
       " 31: 'r',\n",
       " 32: '[n-]'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CC(C)(C)C1CCCN(C(=O)C2=CN=CN=C2)CC1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = pd.read_csv(\"datasets/Chembridge-imageID-SMILE.csv\",names=[\"imgid\",\"smile\"])\n",
    "csv_file[\"imgid\"][1600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attentionmox"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess词典构建\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "\n",
    "def init_from_file():\n",
    "    csv_file = pd.read_csv(\"datasets/Chembridge-imageID-SMILE.csv\",names=[\"imgid\",\"smile\"])\n",
    "    smile_list = list(csv_file[\"smile\"])\n",
    "    ids = list(csv_file[\"imgid\"])\n",
    "    return smile_list,ids\n",
    "    \n",
    "    \n",
    "def tokenize(smiles):\n",
    "    regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "    smiles = replace_halogen(smiles)\n",
    "    char_list = re.split(regex, smiles)\n",
    "    tokenized = []\n",
    "    for char in char_list:\n",
    "        if char.startswith('['):\n",
    "            tokenized.append(char)\n",
    "        else:\n",
    "            chars = [unit for unit in char]\n",
    "            [tokenized.append(unit) for unit in chars]\n",
    "    tokenized.append('END')\n",
    "    return tokenized\n",
    "    \n",
    "def replace_halogen(string):\n",
    "    \"\"\"Regex to replace Br and Cl with single letters\"\"\"\n",
    "    br = re.compile('Br')\n",
    "    cl = re.compile('Cl')\n",
    "    string = br.sub('R', string)\n",
    "    string = cl.sub('L', string)\n",
    "    return string\n",
    "  \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "        \n",
    "def build_vocab():\n",
    "    words = []\n",
    "    smilelist,_ = init_from_file()\n",
    "    for smiles in smilelist:\n",
    "        token = tokenize(smiles)\n",
    "        for tk in token:\n",
    "            words.append(tk)\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab = build_vocab()\n",
    "    with open(\"datasets/image/total_vocab.pkl\",'wb') as f:\n",
    "        pickle.dump(vocab,f)\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class smile_dataset_loader(Dataset):\n",
    "    def __init__(self,vocab,transform=None):\n",
    "        csv_file = pd.read_csv(\"datasets/image/img2smile.csv\")\n",
    "        smile_list = list(csv_file[\"smile\"])\n",
    "        ids = list(csv_file[\"imgid\"])\n",
    "        self.data=smile_list\n",
    "        self.ids = ids\n",
    "        self.vocab =vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        data_id = self.ids[index]\n",
    "        caption = self.data[index]\n",
    "        vocab = self.vocab\n",
    "        image = Image.open(os.path.join(\"datasets/image/train/\",str(index+1)+\".png\"))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        tokens = tokenize(caption)\n",
    "        captions = []\n",
    "        captions.append(vocab('<start>'))\n",
    "        captions.extend([vocab(token) for token in tokens])\n",
    "        captions.append(vocab('<end>'))\n",
    "        target = torch.Tensor(captions)\n",
    "        return image, target\n",
    "            \n",
    "\n",
    "    \n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(smiles):\n",
    "        regex = '(\\[[^\\[\\]]{1,10}\\])'\n",
    "        smiles = replace_halogen(smiles)\n",
    "        char_list = re.split(regex, smiles)\n",
    "        tokenized = []\n",
    "        for char in char_list:\n",
    "            if char.startswith('['):\n",
    "                tokenized.append(char)\n",
    "            else:\n",
    "                chars = [unit for unit in char]\n",
    "                [tokenized.append(unit) for unit in chars]\n",
    "        tokenized.append('END')\n",
    "        return tokenized\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "    images = torch.stack(images, 0)\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "    \n",
    "def get_loader(vocab, transform):\n",
    "    data = smile_dataset_loader(vocab=vocab,transform=transform)\n",
    "    data_lodaer = torch.utils.data.DataLoader(dataset=data,batch_size=40,\n",
    "                                                 shuffle=True,\n",
    "                                                 collate_fn=collate_fn)\n",
    "    return data_lodaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "with open(\"datasets/image/total_vocab.pkl\", 'rb') as f:\n",
    "    vocab_train = pickle.load(f)\n",
    "data_loading = get_loader(vocab_train,transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "for i,(imgs, caps, caplens) in enumerate(data_loading):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "\n",
    "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                    bleu4, is_best):\n",
    "    \"\"\"\n",
    "    Saves model checkpoint.\n",
    "    :param data_name: base name of processed dataset\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param bleu4: validation BLEU-4 score for this epoch\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'epochs_since_improvement': epochs_since_improvement,\n",
    "             'bleu-4': bleu4,\n",
    "             'encoder': encoder,\n",
    "             'decoder': decoder,\n",
    "             'encoder_optimizer': encoder_optimizer,\n",
    "             'decoder_optimizer': decoder_optimizer}\n",
    "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
    "    torch.save(state, filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, 'BEST_' + filename)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    \"\"\"\n",
    "    Shrinks learning rate by a specified factor.\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "\n",
    "def accuracy(scores, targets, k):\n",
    "    \"\"\"\n",
    "    Computes top-k accuracy, from predicted and true labels.\n",
    "    :param scores: scores from the model\n",
    "    :param targets: true labels\n",
    "    :param k: k in top-k accuracy\n",
    "    :return: top-k accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
    "    return correct_total.item() * (100.0 / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def cuda_variable(tensor):\n",
    "    if torch.cuda.is_available():\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, encoded_image_size=14):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=False)\n",
    "        resnet.load_state_dict(torch.load(\".torch/models/resnet152-b121ed2d.pth\"))\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        self.fine_tune()\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        out = self.resnet(images)\n",
    "        out = self.adaptive_pool(out)\n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=True):\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)   # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights\n",
    "        \"\"\"\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        embeddings = self.embedding(encoded_captions)\n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths = [c-1 for c in caption_lengths]\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths ])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "\n",
    "#from model import EncoderCNN, AttnDecoderRNN\n",
    "\n",
    "#from data_loader import get_loader\n",
    "#from nltk.translate.bleu_score import corpus_bleu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def train():\n",
    "    transform = transforms.Compose([ \n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "    with open(\"datasets/image/total_vocab.pkl\", 'rb') as f:\n",
    "        vocab_train = pickle.load(f)\n",
    "    data_loading = get_loader(vocab_train,transform)\n",
    "    encoder = EncoderCNN().to(device)\n",
    "    decoder = AttnDecoderRNN(512,512,512,len(vocab_train)).to(device)\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    criterions = nn.CrossEntropyLoss()\n",
    "    encoder.fine_tune(False)\n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),lr=0.001)\n",
    "    losses = 1000\n",
    "    for epoch in range(100):\n",
    "        for i, (imgs, caps, caplens) in enumerate(data_loading):\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            imgs = encoder(imgs)\n",
    "            \n",
    "            scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "            targets = caps_sorted[:, 1:]\n",
    "            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "            \n",
    "            loss = criterions(scores, targets)\n",
    "            \n",
    "            decoder_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #losses = loss.item()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if losses<loss.item():\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'newdecoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'newencoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            if i%5 ==0:\n",
    "                 print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch+1, 10, i+1, 9000, loss.item(), np.exp(loss.item()))) \n",
    "        '''    if i % 9000==0:\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'newdecoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    \"datasets/images\", 'newencoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "            \n",
    "            \n",
    "            \n",
    "        '''   \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "\n",
    "from model import EncoderCNN, AttnDecoderRNN\n",
    "\n",
    "from data_loader import get_loader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from utils import *\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_name', type=str, default='coco_5_cap_per_img_5_min_word_freq')\n",
    "parser.add_argument('--model_path', type=str, default='models/' , help='path for saving trained models')\n",
    "parser.add_argument('--crop_size', type=int, default=224 , help='size for randomly cropping images')\n",
    "parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')\n",
    "parser.add_argument('--image_dir', type=str, default='data/resized2014', help='directory for resized images')\n",
    "parser.add_argument('--image_dir_val', type=str, default='data/val2014_resized', help='directory for resized images')\n",
    "parser.add_argument('--caption_path', type=str, default='data/annotations/captions_train2014.json', help='path for train annotation json file')\n",
    "parser.add_argument('--caption_path_val', type=str, default='data/annotations/captions_val2014.json', help='path for val annotation json file')\n",
    "parser.add_argument('--log_step', type=int , default=100, help='step size for prining log info')\n",
    "parser.add_argument('--save_step', type=int , default=1000, help='step size for saving trained models')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument('--embed_dim', type=int , default=512, help='dimension of word embedding vectors')\n",
    "parser.add_argument('--attention_dim', type=int , default=512, help='dimension of attention linear layers')\n",
    "parser.add_argument('--decoder_dim', type=int , default=512, help='dimension of decoder rnn')\n",
    "parser.add_argument('--dropout', type=float , default=0.5)\n",
    "parser.add_argument('--start_epoch', type=int, default=0)\n",
    "parser.add_argument('--epochs', type=int, default=120)\n",
    "parser.add_argument('--epochs_since_improvement', type=int, default=0)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--num_workers', type=int, default=1)\n",
    "parser.add_argument('--encoder_lr', type=float, default=1e-4)\n",
    "parser.add_argument('--decoder_lr', type=float, default=4e-4)\n",
    "parser.add_argument('--checkpoint', type=str, default='ckpt/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar' , help='path for checkpoints')\n",
    "parser.add_argument('--grad_clip', type=float, default=5.)\n",
    "parser.add_argument('--alpha_c', type=float, default=1.)\n",
    "parser.add_argument('--best_bleu4', type=float, default=0.)\n",
    "parser.add_argument('--fine_tune_encoder', type=bool, default='False' , help='fine-tune encoder')\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        imgs = encoder(imgs)\n",
    "\n",
    "        # scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        loss = criterion(scores, targets)\n",
    "        loss += args.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if args.grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, args.grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, args.grad_clip)\n",
    "\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % args.log_step == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time, loss=losses,\n",
    "                                                                          top5=top5accs))\n",
    "\n",
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :return: BLEU-4 score\n",
    "    \"\"\"\n",
    "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "    hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
    "\n",
    "        # Move to device, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        if encoder is not None:\n",
    "            imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas = decoder(imgs, caps, caplens)\n",
    "\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores_copy = scores.clone()\n",
    "        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += args.alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        if i % args.log_step == 0:\n",
    "            print('Validation: [{0}/{1}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
    "                                                                            loss=losses, top5=top5accs))\n",
    "\n",
    "        # Store references (true captions), and hypothesis (prediction) for each image\n",
    "        # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "        # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "\n",
    "        # References\n",
    "        # allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
    "        for j in range(allcaps.shape[0]):\n",
    "            img_caps = allcaps[j].tolist()\n",
    "            img_captions = list(\n",
    "                map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
    "                    img_caps))  # remove <start> and pads\n",
    "            references.append(img_captions)\n",
    "\n",
    "        # Hypotheses\n",
    "        _, preds = torch.max(scores_copy, dim=2)\n",
    "        preds = preds.tolist()\n",
    "        temp_preds = list()\n",
    "        for j, p in enumerate(preds):\n",
    "            temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
    "        preds = temp_preds\n",
    "        hypotheses.extend(preds)\n",
    "\n",
    "        assert len(references) == len(hypotheses)\n",
    "\n",
    "    # Calculate BLEU-4 scores\n",
    "    bleu4 = corpus_bleu(references, hypotheses, emulate_multibleu=True)\n",
    "\n",
    "    print(\n",
    "        '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
    "            loss=losses,\n",
    "            top5=top5accs,\n",
    "            bleu=bleu4))\n",
    "\n",
    "    return bleu4\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=False)\n",
    "        resnet.load_state_dict(torch.load(\".torch/models/resnet152-b121ed2d.pth\"))\n",
    "        modules = list(resnet.children())[:-2]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(2048, 100)\n",
    "        \n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.linear(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en2 = EncoderCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderCNN(\n",
      "  (resnet): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (6): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (7): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (8): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (9): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (10): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (11): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (12): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (13): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (14): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (15): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (16): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (17): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (18): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (19): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (20): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (21): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (22): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (23): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (24): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (25): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (26): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (27): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (28): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (29): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (30): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (31): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (32): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (33): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (34): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (35): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "print(en2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
